{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we shall create a multi-layer perceptron (MLP) that performs the XOR logical operation."
      ],
      "metadata": {
        "id": "CQPDr_hbi2Pz"
      },
      "id": "CQPDr_hbi2Pz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If running this notebook in Colab, please ensure that your Hugging Face `HF_TOKEN` is added to your Colab secrets.\n",
        "\n",
        "Alternatively, please login to Hugging Face by running the following cell."
      ],
      "metadata": {
        "id": "zpIoSGUbjSwU"
      },
      "id": "zpIoSGUbjSwU"
    },
    {
      "cell_type": "code",
      "source": [
        "# !huggingface-cli login"
      ],
      "metadata": {
        "id": "iKueKTWgjThS"
      },
      "id": "iKueKTWgjThS",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create two column vectors containing `0`s and `1`s."
      ],
      "metadata": {
        "id": "eD70mEdcjxMc"
      },
      "id": "eD70mEdcjxMc"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "batch = {'a': torch.tensor([[0.], [0.], [1.], [1.]]), 'b': torch.tensor([[0.], [1.], [0.], [1.]])}\n",
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKwllwhgormn",
        "outputId": "de473bcc-076a-4094-9337-4185a0aa59f8"
      },
      "id": "cKwllwhgormn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a': tensor([[0.],\n",
              "         [0.],\n",
              "         [1.],\n",
              "         [1.]]),\n",
              " 'b': tensor([[0.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]])}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our objective is to generate the following truth table using an MLP.\n",
        "\n",
        "| A | B | C |\n",
        "| - | - | - |\n",
        "| 0 | 0 | 0 |\n",
        "| 0 | 1 | 1 |\n",
        "| 1 | 0 | 1 |\n",
        "| 1 | 1 | 0 |"
      ],
      "metadata": {
        "id": "5HUIzQj8j1rf"
      },
      "id": "5HUIzQj8j1rf"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "\n",
        "class XOR(nn.Module, PyTorchModelHubMixin):\n",
        "    \"\"\"\n",
        "    A multi-layer perceptron that performs the XOR logical computation.\n",
        "    It takes as input two column vectors of zeros and ones. It outputs a single column vector of zeros and ones.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer0_weight = torch.tensor([[1., 1.], [1., 1.]])\n",
        "        self.layer0_bias = torch.tensor([-1.5, -0.5])\n",
        "        self.layer1_weight = torch.tensor([[-1.], [1.]])\n",
        "        self.layer1_bias = torch.tensor([-0.5])\n",
        "\n",
        "    def heaviside(self, x):\n",
        "        return (x >= 0).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs = torch.cat([x['a'], x['b']], dim=1)\n",
        "        out = self.heaviside(inputs @ self.layer0_weight + self.layer0_bias)\n",
        "        out = self.heaviside(out @ self.layer1_weight + self.layer1_bias)\n",
        "        return out"
      ],
      "metadata": {
        "id": "FU5z1k5vqhDV"
      },
      "id": "FU5z1k5vqhDV",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XOR()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m0NXv9DsGHD",
        "outputId": "ce87d617-b498-49de-df1a-ff94954fa903"
      },
      "id": "7m0NXv9DsGHD",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XOR()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward pass:"
      ],
      "metadata": {
        "id": "_Z4u0O3RjeBy"
      },
      "id": "_Z4u0O3RjeBy"
    },
    {
      "cell_type": "code",
      "source": [
        "model(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJIIiUU_sIi9",
        "outputId": "4b73e806-4dd9-44d7-be46-a8b957623d88"
      },
      "id": "pJIIiUU_sIi9",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The XOR operation works.\n",
        "\n",
        "Let's push our model to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "5HQUuFpPjhpR"
      },
      "id": "5HQUuFpPjhpR"
    },
    {
      "cell_type": "code",
      "source": [
        "# model.push_to_hub(\"xor\", commit_message=\"pushing XOR\")"
      ],
      "metadata": {
        "id": "AnK70ptShoFK"
      },
      "id": "AnK70ptShoFK",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5dn833UsOgN"
      },
      "id": "U5dn833UsOgN",
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}